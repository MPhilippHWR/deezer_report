---
title: 'Report: Deezer Kaggle Inclass Competition 2017'
author: "Christian Kregelin, Dennis Uckel, Max Philipp, Pranav Pandya and Vera Weidmann "
date: "14. April - 31. May 2017"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
---

##1. Introduction

This project is based on the data science games 2017. It is a worldwide competition between universities. This year the data was provided by the music streaming application Deezer. They offer a recommendation feature, called “Flow”, which suggests the user songs they might like to listen to. The algorithm behind "Flow" uses collaborative filtering to provide the user with the right music at the right time. While the algorithm detects the user's current listing taste by analysing the current listened and current skipped songs to improve the recommendations, the first song prediction is the most difficult one. Therefore, the goal of this challenge was to predict how likely it is that the users of the test dataset will listen to the first track of Flow. 

In the end of the project we achieved the place 66 out of 145 teams with 45 submissions and a score of 0.63860. This report reflects our whole project journey, including our ideas, failures and achievements. 

Therefore, the report will give a short introduction to the dataset at first. Sequentially, our approaches in the field of feature engineering are explained and the used prediction models are presented.  

The libraries were used for the project are the following: 
```{r Initialization, echo=FALSE, message=FALSE, warning=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(dplyr,xgboost, Matrix,data.table, caret, splitstackshape, ggplot2, psychometric, RColorBrewer, jsonlite, httr, corrplot)
#library(ggplot2)
#library(dplyr)
#library(RColorBrewer)
```

##2. Data Analysis & Challenges
This chapter illustrates an exploratory analysis of the competition dataset.
As it is mentioned before, the goal of this challenge was to predict whether the first recommended song will be listened or not. Therefore, the test dataset contains the first recommended track for several different users. The train dataset was generated by the user's listening history for one month. Each row represents one listened or not listened song. 

Data:
```{r}
Deezer <- read.csv("/home/Deezer/10_Basic_Dataset/train.csv")
Deezer_test <- read.csv("/home/Deezer/10_Basic_Dataset/test.csv")

tmp <- Deezer_test[,-1] 
tmp$is_listened <- 0
all <- rbind(Deezer,tmp)
rm(tmp)
```
The train data contains `r round(dim(Deezer)[1]/1000000,2)` million rows and `r dim(Deezer)[2]` features, while the test set just contains `r round(dim(Deezer_test)[1]/1000,2)` thousand rows.

The dataset provides the following columns, which we grouped in user specific, song specific and device specific features. These features are analyzed as well detections of NAs are conducted. As all features are characterized as numeric, we convert them into the right data type later on. 

User specific: 
  * user_id -  anonymized id of the user
  * user_gender -  gender of the user
  * user_age - age of the user
  
Song specific:
  * media_id - identifiant of the song listened by the user
  * album_id - identifiant of the album of the song
  * artist_id - identifiant of the artist of the song
  * genre_id - identifiant of the genre of the song
  * media_duration - duration of the song
  * context_type - type of content where the song was listened: playlist, album ...
  * release_date - release date of the song with the format YYYYMMDD

Device specific: 
  * platform_name - type of os
  * platform_family - type of device
  
Other features:
  * ts_listen - timestamp of the listening in UNIX time
  * listen_type - if the songs was listened in a flow or not
  
Response variable:
  * is_listened - 1 if the track was listened longer than 30 seconds, 0 otherwise

####2(a) User specific features

To data contains `r length(unique(Deezer$user_id))` different users. Some of them occur over thousands of times, while nearly 1500 users arise just once (with one listened or non listened song). Non NAs are found. 

```{r, warning=FALSE}
user_distribution <- Deezer %>%
  group_by(user_id) %>%
  summarise(n=n()) %>%
  arrange (-n)
myColors <- rep(brewer.pal(7,"Blues")[5:6],50)

sum(is.na(user_distribution$user_id))

(gg6 <-ggplot(user_distribution, aes(x=n))+
  geom_histogram(bins=100, fill=myColors,color="black")+
  scale_x_continuous(breaks=seq(0,2000,50),limits=c(0,2000))+
labs(subtitle="in the train set",
      y="number of users",
      x="number of songs",
      title="Distribution of Observations per User",
      caption="Source: Deezer train data")+
theme_light()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1)))
#ggsave("observations_distribution.png",plot=gg6, width = 8, height=5, units="in")
```
  
The data reflects the listening behavior of male and female users. However, in this project we do not know how the values of 0 and 1 belong the specific genders. The plot below illustrates the amount of users per gender. 

```{r}
plot(as.factor(Deezer$user_gender))
```

The user's age is in the range of `r min(Deezer$user_age)` and `r max(Deezer$user_age)`. The following boxplot illustrates the distribution of the unique users by their age. Accordingly, 50% of the users are 24 years old or younger. The feature age does not contain any NAs in the Deezer dataset.

```{r}
round(median(Deezer$user_age))
age <- Deezer %>%
  group_by(user_id) %>% summarise(age=max(user_age))
boxplot(age$age)

sum(is.na(Deezer$user_age))
```

not needed anymore: (->max??)
```{r}
age <- Deezer %>%
  group_by(user_id) %>% summarise(age=max(user_age))

myColors <- c(rep(brewer.pal(7,"Blues")[5:6],6),"#4292C6")
(gg7 <- ggplot(age, aes(x=age))+stat_bin(binwidth=1, fill=myColors, color="black") +  scale_y_continuous(limits=c(0,2200),breaks=seq(0,2200,100))+
    stat_bin(binwidth=1, geom="text", aes(label=..count..), vjust=-1.5)+ scale_x_continuous(breaks=seq(18,30,1))+
labs(y="number of users",
      x="age",
      title="Distribution of Age among Users",
      caption="Source: Deezer train data")+
theme_light()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1)))
#ggsave("Age_distribution.png",plot=gg7, width = 8, height=5, units="in")
```

#### 2(b) Song specific features

The dataset contains `r length(unique(Deezer$media_id))` different media_ids. One media_id presents one specific song from one artist on one specific album. At first, we thought that one media_id reflects one unique song. However, while checking the provided extra information (extra_infos.json), which links each media_id to a song title, album title and an artist name, it can be seen that one unique song (e.g. Everybody from Backstreet Boys) can have more than one media_id. This is caused by the album. Whenever a song is also on another album (e.g. Fetenkult - Best of the 90's) a new media_id is generated. 

One example is covered by the following data table: 
```{r}
extra = stream_in(file("/home/Deezer/10_Basic_Dataset/extra_infos.json"))
extra_example <- extra %>%
  filter(sng_title=="Everybody (Backstreet's Back)" & art_name=="Backstreet Boys")
data.table(extra_example)
```

Furthermore, we can count `r length(unique(Deezer$album_id))` different albums and `r length(unique(Deezer$artist_id))` different artists in the dataset. No NAs are identified. 

```{r}
sum(is.na(Deezer$media_id))
sum(is.na(Deezer$album_id))
sum(is.na(Deezer$artist_id))
```

According to the extra information from the json file, we know that media_id and album_id are strongly correlated with each other. The correlation coefficient in that case is: `r cor(Deezer$media_id,Deezer$album_id)`.
The following histogram underlines this correlation. Furthermore, some media_ids (and therefore some albums and artists) are recommended quite often over all users. (??maybe not necessarily important to show these plots...)

```{r}
par(mfrow=c(1,3))
hist(Deezer$media_id)
hist(Deezer$album_id)
hist(Deezer$artist_id)
```

Likewise, `r length(unique(Deezer$genre_id))` different genres exist in the dataset. The histogram shows clearly that one specific genre with id = 0 occurs 3 million times while the second one for example just occurs not even 1 million times. 
This would mean that one specific genre is recommended most of the time. However, further investigations are done in the feature engineering chapter. 

```{r}
sum(is.na(Deezer$genre_id))

number_genre <- Deezer %>%
  group_by(genre_id) %>%
  summarize(n=n()) %>%
  arrange(-n)
head(number_genre)

myColors <- c(rep(brewer.pal(7,"Blues")[5:6],50))

hist(Deezer$genre_id)

gg14 <-ggplot(Deezer, aes(x=genre_id))+geom_density(fill="#4292C6")+scale_x_continuous(limits=c(0,100))+
labs(y="logarithmic observations",
      x="genre_id",
      title="Logarithmic Distribution of Genre_id",
      caption="Source: Deezer train data")+
theme_light()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggsave("distribution_original_genre.png",plot=gg14, width = 8, height=5, units="in")
```

  

(?) media_duration - duration of the song
  
  
```{r}
sum(is.na(Deezer$media_duration))
hist(Deezer$media_duration)
```


(?) context_type - type of content where the song was listened: playlist, album ...
  
  
```{r}
sum(is.na(Deezer$context_type))
hist(Deezer$context_type)
data.table(table(Deezer$context_type))
```

  
(?) release_date - release date of the song with the format YYYYMMDD
```{r}
sum(is.na(Deezer$release_date))
```


#### 2(c) Device specific features

The dataset contains also information about the device and the operating system the user used for listening. Both features have 3 values (1:3). As it seems that device 1 and the operating system 1 is used the most, we cannot say which specific device or system it is. Furthermore, these both columns are correlated (r=`r round(cor(Deezer$platform_family, Deezer$platform_name),2)`) 

```{r}
par(mfrow=c(1,2))
plot(as.factor(Deezer$platform_family))
plot(as.factor(Deezer$platform_name))

sum(is.na(Deezer$platform_family))
sum(is.na(Deezer$platform_name))
```

#### 2(d) Other features, incl. response variable

The ts_listen feature includes the time information when a specific song was recommended to a user. It is a UNIX format and will be modified later on. Non NAs are detected. 

```{r}
sum(is.na(Deezer$ts_listen))
```

As the training data covers the overall listening behavior of an user, the song history includes songs which were listened in the flow and songs which were not recommended (listened by search or saved song lists). Thereby, `r round(table(Deezer$listen_type)[1]/1000000,2)` million rows do not presents recommended songs, while `r round(table(Deezer$listen_type)[2]/1000000,2)` were predicted by the recommendation engine Flow. The test data contains just predicted songs by Flow. 

```{r}
plot(as.factor(Deezer$listen_type))
table(Deezer_test$listen_type) # 1 wrong entry?? hä
sum(is.na(Deezer$listen_type))
```

At last, the most important column "is_listened" presents the response variable in the modeling part. 
```{r}
plot(as.factor(Deezer$is_listened))
sum(is.na(Deezer$is_listened))
```

#### 2(e) Summary of main challenges

At the first glimpse, the dataset looks like tall data. However, most of the features are factors. As already mentioned in the previous chapter, the data contains `r length(unique(Deezer$user_id))` different users, `r length(unique(Deezer$genre_id))` genres, `r length(unique(Deezer$artist_id))` artists, `r length(unique(Deezer$album_id))` albums and `r length(unique(Deezer$media_id))` unique songs. 

Therefore, the main challenge of that project was how to create a prediction model with this amount of factors. Furthermore, insufficient data of the column genre_id and a to grained timestamp and release date were the features which needed some improvements. 

Furthermore, the following correlation plot visualizes the collinearity between the variables. 
```{r}
corDeezer <- cor(Deezer)
#png("correlationMatrix.png")
corDeezerplot <- corrplot(corDeezer, method = "ellipse", type = "full")
#dev.off()
```

##3. Feature Engineering

In this part, we will talk about our Feature Engineering. We will start with easier transformatings and calculations going into advanced thinking processes throughout this chapter. We tried to demonstrate all we have archived and how we got chronologically to the point of most success.

#### 3(a) Timestamp conversion

Converting the Timestamp is the most basic feature engineering we have done. We extract Hour of Day as well as weekday from the timestamp. This procedure is so easy, that we did it on the fly, as you will see at the end of the feature engineering section. Nethertheless, here is what will happen.


```{r}
transforming_timestamp <- function(x){
timestamp = as.POSIXct(x$ts_listen, origin="1970-01-01")
  splitdt  <- data.frame(
    hh = as.numeric(format(timestamp, format = "%H")), #24hours format
    wd = as.numeric(format(timestamp, format = "%w"))) #weekday
  x = cbind(x, splitdt) #cbind the extracted time to data
  return(x)
}
transforming_timestamp(Deezer)[1:10,12:17]
```

```{r}
tmp <- transforming_timestamp(Deezer)
myColors <- rep(brewer.pal(7,"Blues")[5:6],12)

gg9 <- ggplot(tmp, aes(x=hh))+stat_bin(binwidth=1,fill=myColors, color="black")+
  scale_y_continuous(limits=c(0,600000),breaks=seq(0,600000,50000))+
  scale_x_continuous(breaks=seq(0,23,1))+
labs(y="observations",
      x="hour of day",
      title="Distribution of Listening Hour",
      caption="Source: Deezer train data")+
theme_light()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggsave("hh_distribution_train.png",plot=gg9, width = 8, height=5, units="in")
```

```{r}
myColors <- c(rep(brewer.pal(7,"Blues")[5:6],3),"#4292C6")

gg10<-  ggplot(tmp, aes(x=wd))+stat_bin(binwidth=1,fill=myColors, color="black")+
  scale_y_continuous(limits=c(0,1300000),breaks=seq(0,1300000,100000))+
  scale_x_continuous(breaks=seq(0,6,1))+
labs(y="observations",
      x="weekday",
      title="Distribution of Listening Weekday",
      caption="Source: Deezer train data")+
theme_light()
ggsave("wd_distribution_train.png",plot=gg10, width = 8, height=5, units="in")
```

```{r}
tmp <- transforming_timestamp(Deezer_test)
myColors <- rep(brewer.pal(7,"Blues")[5:6],12)

gg11 <- 
  ggplot(tmp, aes(x=hh))+stat_bin(binwidth=1,fill=myColors, color="black")+
  scale_y_continuous(limits=c(0,1800),breaks=seq(0,1800,100))+
  scale_x_continuous(breaks=seq(0,23,1))+
labs(y="observations",
      x="hour of day",
      title="Distribution of Listening Hour",
     subtitle="for test data",
      caption="Source: Deezer test data")+
theme_light()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggsave("hh_distribution_test.png",plot=gg11, width = 8, height=5, units="in")
```

```{r}
myColors <- c(rep(brewer.pal(7,"Blues")[5:6],3),"#4292C6")

gg12<-  ggplot(tmp, aes(x=wd))+stat_bin(binwidth=1,fill=myColors, color="black")+
  scale_y_continuous(limits=c(0,5000),breaks=seq(0,5000,250))+
  scale_x_continuous(breaks=seq(0,6,1))+
labs(y="observations",
      x="weekday",
      title="Distribution of Listening Weekday",
     subtitle="for test data",
      caption="Source: Deezer test data")+
theme_light()
ggsave("wd_distribution_test.png",plot=gg12, width = 8, height=5, units="in")
```

#### 2(b) Beats per Minute

Beats per Minute (bpm) is an easy, numeric feature which we could query from the API. We thought it may help us differentiate inside of genres for example "softrock" being tendentially slower (lower bpm) than its assosiated genre "rock". It wasn´t our intention to create subgenres in any way, but just giving this extra information into the model.

For quering the API we used the httr package for the query and jsonlite to handle the received answer. To show how the code works, we limit the number of queries to 50 instead of nearly 450.000 media_ids

```{r}
## Load necessary packages
library(jsonlite)
library(httr)
uniquetracks <- as.data.frame(unique(all$media_id)[1:50]) #getting all unique media_ids
uniquetracks$api <- paste("https://api.deezer.com/track/",uniquetracks[,1], sep="")
uniquetracks$bpm <- 0

for (i in 1:length(uniquetracks[,1])) { ## for all tracks
  this.raw.result <- GET(url = as.character(uniquetracks[i,2])) ## get the infos
  this.result <- fromJSON(rawToChar(this.raw.result$content)) ## turn it into a readable format
  uniquetracks$bpm[i] <- ifelse(is.null(this.result$bpm),"NA",this.result$bpm) ## get the BPM
#  message(as.character(i), appendLF = FALSE) ## print the iteration to see if the code is still working
  Sys.sleep(time = 0.05) ## cap the speed so the 50 per 5 seconds are not violated
}
head(uniquetracks,10)
#save(tracks, file = "tracks_BPM.rda")
```

After getting the information for the API, we need to clean it. Because the bpm is received as character, we can set all cells were the string is "NA" to 0 without using is.na(). Transforming them to numeric afterwards leads to 0 missing data, but 30000 zeros (only 171 were introduced by setting "NA" to 0).

```{r warning=FALSE}
load("/home/Deezer/30_Wrangled_Data/Archiv/tracks_BPM.rda")
tracks <- tracks[,c(2,3)]
tracks$track_bpm[tracks$track_bpm=="NA"] <- "0"
tracks$track_bpm <- as.numeric(tracks$track_bpm)
hist(as.numeric(tracks$track_bpm))
```

To fill the zeros, we will take the mean bpm from the album the media is on, or the genre it is from, if die album information is not available. Sadly, at this point we didn´t had informations about the genre, so in order to keep it chronologically correct and not confusing, we will go to the genre section now and will come back to this afterwards.


#### 2(c) Genre_id

In a first approach we tried to get enhanced information about the genre_id, by quering the API for the genre_ids.
As seen in the visualisation part, we were confronted with a vast amount of genre_id==0 and it would be nice to know which genre it is.

```{r}
uniquegenres <- unique(all$genre_id) #getting all unique genre_ids
uniquegenres <- as.data.frame(paste("https://api.deezer.com/genre/",uniquegenres, sep="")) #paste the ids into the needed API-url to access the informations
uniquegenres$name <- "" #initialise a empty column
uniquegenres$genre_id <- unique(all$genre_id)
```

At this point we are able to query from the api, using the following loop. To show the results, we set the sample size to 50.

```{r}
library(httr)
library(jsonlite)

for (i in 1:50){ #first 500 genres
this.raw.result <- GET(url = as.character(uniquegenres[i,1])) # get data
this.result <- fromJSON(rawToChar(this.raw.result$content)) # turn data from unix to char and from json into a variable
uniquegenres$name[i] <- ifelse(is.null(this.result$name),"NA",this.result$name) # fill NA where NULL
Sys.sleep(time = 0.1) # System Sleep time to not overload the APIs capacity of 50 requests every 5 seconds
}
head(uniquegenres[,c(2,3)],10)

```
As we can see, the genre names are mainly NAs, and if one would look at row number 21, we would see, that genre_id==0 refers to "all". 

Now that we know, that we can and should not rely on the genre_id, we needed to get the information about the genre from another source. Luckily, we have the album_id and the API provides genre information for each album individually, which is not related to the genre_id in first place.

#####Genre Information from album_id
```{r}
albums <- unique(all$album_id) #getting all album ids
albums <- paste("https://api.deezer.com/album/",albums, sep="") #get them in the right format for query the API
albums <- as.data.frame(albums)
albums$album_id <- unique(all$album_id)

#initiating some columns
albums$alb.genre <- ""
albums$alb.genre.id <- ""
```

In the next chunk we receive the genre information for the first 50 albums.
```{r}
for (i in 1:50){ #for 50 albums. you can replace 50 by length(albums$albums) to loop over all
this.raw.result <- GET(url = as.character(albums[i,1])) #get the infos
this.result <- fromJSON(rawToChar(this.raw.result$content)) #turn it into a readable format
albums$alb.genre[i] <- ifelse(is.null(this.result$genres$data[2]),"NA",this.result$genres$data[2]) #getting the written genre
albums$alb.genre.id[i] <- ifelse(is.null(this.result$genre_id),"NA",this.result$genre_id) #getting the genre_id from the album
#message(as.character(i), appendLF = FALSE) #print the iteration to see if the code is still working
Sys.sleep(time = 0.05) #cap the speed so the 50 per 5 seconds are not violated
}

head(albums$alb.genre)
```

So far we created a new DF called "albums" which has 4 columns: API-query, album_id, genre and genre_id.
Where genre is the written name of the genre and the id is just the corresponding id.
We can see, that some album have more than one genre, and some have NULL.

Looking back at the data, it would be quite easy to get the lists of genre out of the cells of the df. But as we started the project, we weren´t that fund of performing transformations on lists of lists. And since this report demonstrate what we have done and not what we should have done, here is our original approach.

We used gsub to get rid of all symbols.

```{r}
albums[,'alb.genre2'] <- gsub("c\\(", "" , albums[,'alb.genre']) #remove "'s
albums[,'alb.genre2'] <- gsub("\\)", "" , albums[,'alb.genre2']) #remove "'s
albums[,'alb.genre2'] <- gsub("\"", "" , albums[,'alb.genre2']) #remove "'s
albums[,'alb.genre2'] <- gsub(" ", "" , albums[,'alb.genre2']) #remove "'s

albums <- splitstackshape::cSplit(albums,splitCols = "alb.genre2",direction = "long")
head(albums$alb.genre2,10)
```

At this stage we decided to only grab the very first of the listed genres for each albums, since it would make most sense, that the main genre is named first. We still have NULLs and NAs.

Our first approach in tackeling the NULLs and NAs was to get the albums corresponding artist ids. Than, with the artist information, we were able to lookup the artist´s most played genre to fill in missing data. For Example if artist 1 had 4 albums in genre "rock", one could assume that a 5th album, which genre is missing is "rock" aswell.

```{r}
albums$artist <- all$artist_id[albums$album_id %in% all$album_id] #joining artist informations

#save(albums,file="bums_art.rda")
#load("bums_art.rda")
```

At this point we need to load in data which has been saved during our running.
```{r}
load("/home/Deezer/30_Wrangled_Data/bums_art.rda")
head(albums,10)
```

Only on with this data you can see what splitstackshape::cSplit really did. It cutted the genres from the list of list and got each of them into a new column. Therefore just using the very first of this new columns is what we want, since it is the first entry of the genres.
To get going we created a new DF which contains all the missing data (NAs and NULLs)

```{r}
Na.albums <- albums %>% filter(alb.genre.id=="NA" | alb.genre.id==-1) #get all NAs and NULLs into seperate DF
Na.albums <- as.data.frame(Na.albums[,c(1,20)])
colnames(Na.albums) <- c("album_id","artist_id")
albums2 <- albums[!albums$album_id %in% Na.albums$album_id,] #get clean data into a new DF
```
as we can see, we have 20521 rows in Na.albums which means that we have 20521 albums out of 151605 which doesnt have a genre after quering the API.

```{r}
all2 <- all %>% select(album_id,artist_id) #subset all
all2 <- left_join(all2,albums2[,c(1,4)],by="album_id") #joining our new genre

#creating artists lookuptable
loup.artists <- all2 %>%
  group_by(artist_id,alb.genre2_01) %>%
  summarise(n=n()) %>% #how often a genre at a artist appears. btw: listened/played songs more important than overall genres from albums released
  filter(n==max(n))

Na.albums <- merge(Na.albums,loup.artists[,-3],by="artist_id") #reduced from 20.000 NA albums to 999.

albums2 <- albums2[,c(20,1,4)]
albums2 <- bind_rows(albums2, Na.albums[!is.na(Na.albums$alb.genre2_01),-3]) #adding albums with new information to album2
Na.albums <- Na.albums[is.na(Na.albums$alb.genre2_01),] #999 albums still without genre
```

This implementation was a big success. We were able to make a valid guess on the genre_id by using the artist´s main genre, reducing the number of NAs and NULLs by more than 95%.

To fill the final 999 genres, we used the bpm, which we have collected earlier to identify the genres, but as mentioned aswell, we need what we know about the genre so far to fill our gaps in bpm knowledge adequatly.

First we get all songs were our bpm is 0, afterwards we first fill in the mean bpm of the album the song is in, and if we dont have the information, we fill with the mean bpm of the genre.

note: make sure you have run 2(b) already

```{r warning=FALSE}
#bpm.data <- all[!duplicated(all$media_id),c("media_id","album_id")]
bpm.data <- unique(all[,c("media_id","album_id")]) #get all media
bpm.data<- merge(bpm.data,tracks,by="media_id") #join bpm
bpm.data<- merge(bpm.data,albums2[,c(-1,-4)],by="album_id", all.x=TRUE) #join genre_id we know so far

#creation if album and genre lookuptables for the everage bpm
bpm.merge.album <- bpm.data %>% filter(track_bpm>0) %>% group_by(album_id) %>% summarise(track_bpm=mean(track_bpm))
bpm.merge.genre <- bpm.data %>% filter(track_bpm>0) %>% group_by(alb.genre2_01) %>% summarise(track_bpm=mean(track_bpm))

#fill in album_mean_bpm where it is known
bpmclean2 <- bpm.data[bpm.data$track_bpm==0,]
bpmclean2 <- merge(bpmclean2[,c(1,2,4)],bpm.merge.album, by="album_id",all.x=TRUE)

bpmclean3 <- bpmclean2[is.na(bpmclean2$track_bpm),] #where we dont have bpm data from the album
bpmclean3 <- merge(bpmclean3[,c(-4)],bpm.merge.genre, by="alb.genre2_01",all.x=TRUE) #join the information from the average_genre_lookup table
```

What was quite amusing is one very resilient little fellow: In all 7.5 mio songs in our data, there is only one "HörbuchaufDeutsch". Since it is the only one of his kind, and he is an albums by himself, we don´t have any information, but we set the bpm to 80, which is on the lower end of the scale.

```{r}
bpmclean3[is.na(bpmclean3$track_bpm),]
bpmclean3[is.na(bpmclean3$track_bpm),4] <- 80
```

Now we only had to clean everything and bind it back into a single bpm lookup table which we will call tracks_cleaned.rda

```{r}
bpm1 <- bpm.data %>% filter(track_bpm>0)
bpm2 <- bpmclean2 %>% filter(!is.na(track_bpm))
bpm2 <- bpm2[,c(1,2,4,3)]
bpm3 <- bpmclean3[,c(2,3,4,1)]

tracks_clean <- rbind(bpm1,bpm2,bpm3)
tracks_clean <- tracks_clean[!duplicated(tracks_clean$media_id),]

tracks_clean2 <- tracks_clean[,c(2,3)]
#save(tracks_clean, file="data_for_genre_loop.rda")
#save(tracks_clean2,file="tracks_cleaned.rda")
```

```{r}
load("/home/Deezer/30_Wrangled_Data/tracks_cleaned.rda")

myColors <- rep(brewer.pal(7,"Blues")[5:6],10)

gg8 <- ggplot(tracks_clean2, aes(x=track_bpm))+stat_bin(bins=20, fill=myColors, color="black")+
  scale_y_continuous(limits=c(0,80000),breaks=seq(0,80000,5000))+
  scale_x_continuous(breaks=seq(50,220,10),limits=c(50,220))+
labs(title="Distribution of Beats per Minute",
     y="observations",
      x="beats per minute",
      caption="Source: Deezer train data")+
theme_light()+
  theme(axis.text.x = element_text(angle = 75, hjust = 1))
ggsave("BPM_distribution.png",plot=gg8, width = 8, height=5, units="in")

```


To fill the missing genre_ids for our 999 observations, we will conduct a glm model to predict the genre by the songs bpm, since we have already exhausted all other possibilities. According to a majority vote on how the genre_id is distributed on the known cases, we could assume, that the missing ones are "pop".

```{r, fig.width=8,fig.height=5}
#load("/home/Deezer/30_Wrangled_Data/Deezer_train_0525.rda")
#insert treemap of genre distribution
library(treemap)
png(filename="genre_treemap.png",width=16, height=9, units = "in",res=72)
treemap(DeezerNew_train_0525 %>% group_by(alb.genre2_01) %>% summarise(n=log(n())), index="alb.genre2_01",vSize = "n",title = "Logarithmic Distribution of Genres", palette = "GnBu")
dev.off()
```

Note that this code will run for some minutes, you can skip the following two chunks without missing something crucial.

```{r}

test.loop <- tracks_clean %>% filter(is.na(alb.genre2_01)) %>% select(album_id,track_bpm)
results <- test.loop

#loop and fit
for (i in levels(tracks_clean$alb.genre2_01)){
tracks_clean$y <- as.factor(as.numeric(tracks_clean$alb.genre2_01==i))
train.loop <- tracks_clean %>% filter(!is.na(alb.genre2_01)) %>% select(track_bpm,y)

fit <- glm(y~track_bpm,data=train.loop, family=binomial(link = "logit"))
results[,i] <- predict(fit,test.loop, type="response")
}
results <- results[,-2]

save(results, file="results.rda")
```

At this point we

```{r}
albums.finish <- melt(results,id.vars="album_id")
albums.finish <- albums.finish %>% group_by(album_id,variable) %>% summarise(value=sum(value))
albums.finish <- albums.finish %>% group_by(album_id) %>% filter(value==max(value))
table(albums.finish$variable)
```
The tables shows that for all 20521 songs from out missing 999 albums, the looped glm model suggested, that all of the songs are so be considered "pop", which also followed the majority vote.

```{r warning=FALSE}
Na.albums$alb.genre2_01 <- "Pop"
albums2 <- bind_rows(albums2[,c(2,3)], Na.albums[,c(2,3)]) #adding albums with new information to album2
albums2 <- albums2[!duplicated(albums2$album_id),]

#save(albums2,file="album_genre_clean.rda")
```


#### 2(d) User clusters by genre

```{r}
load("/home/Deezer/30_Wrangled_Data/Deezer_train_0525.rda") #we load a newer version of the data at this stage, but we only build on what has been worked out so far.
DeezerNew_train_0525$is_listened <- as.numeric(DeezerNew_train_0525$is_listened)-1
library(dplyr)
library(splitstackshape)
library(reshape2)
```

One of our first thoughts about this project was: "the genre is the most important indicator if you like a song or not."
At this point in time we already had trustworthy information about the genre, so we could feature engineer further.
In a first attempt we generated a clustering based on genre-listening-history. Sadly we cannot deliver the original code, because the Zeno Server crushed after running the code and deleted the at this point unsaved rmd. Luckily, we saved the output of the code in a rda.

Here is a attempt to recreate the code to give you an impression:

```{r}
profs <- DeezerNew_train_0525 %>% 
  group_by(user_id,alb.genre2_01) %>% #for each user and each genre
  summarise(c = sum(is_listened)) %>% #is_listened is 0/1 coded as.numeric, so sum() works fine
  mutate(p = c/sum(c)) #summarise ungroups once, so at this point we are at groub_by(user_id), therefore sum(c)==sum(c | user(i))

profs <- profs[,-3] %>%
  dcast(user_id ~ alb.genre2_01, value.var="p") #dcast is the invers of melt, but creating a new column for each value pair.
profs[is.na(profs)] <- 0
profs[1:10,c(1,3,13,17,29,41)]
```
This chunk gave us the opportunity to look how genres are distributed for every user individual. e.g. User 1 listened 11% "Alternative", 12% "Dance", 24% Electro and so on.

We used this information to create clusters of similar users, using hclust over kmeans, because we didnt know how many clusters we wanted to come up with and didn´t wanted to rerun the code multiple times.

The code looked somewhat like this, note that creating the distance-matrix will take around 2 minutes on the zeno-server and around 5-10 minutes on a regular machine, since it consists of 198 mio elements.

```{r}
d <- dist(profs[,-1])
```

We want to use this journey back into out beginnings to show how different clustering methods looked like. Finally tho, we used the method "complete" and set the cutoff point to 35 clusters.

```{r}
c.tree.comp = hclust(d,method="complete")

w.tree.comp = hclust(d,method="ward.D2")

s.tree.comp = hclust(d,method="single")
```

```{r fig.width=8}
par(mfrow=c(1,3)) 
plot(c.tree.comp)
plot(w.tree.comp)
plot(s.tree.comp)

labs.comp = cutree(c.tree.comp,k=35)
#lookup <-as.data.frame(profs$user_id)
#lookup$profile_id <- labs.comp
#colnames(lookup) <- c("user_id","profile_id")
```

This new feature added to our previously best model increased our score by 4%.

We didn´t stop at this point, since there are some aspects which are worrying. e.g. that we dont take into account the number of observations or how often a genre was played, in contrast, we only look at how often a user has listened to a genre. We kept on developing the idea, which is worth a chapter on its own.

#### 2(e) User Behavior Index

```{r}
load("/home/Deezer/30_Wrangled_Data/Deezer_train_0525.rda")
DeezerNew_train_0525$is_listened <- as.numeric(DeezerNew_train_0525$is_listened)-1
library(dplyr)
library(splitstackshape)
library(reshape2)
```

Sometimes a simple idea can lead to a cascade of thoughts and breakthroughs, as happen in our case.
The thought was a simple one: "When we look at the distribution of genres for each unique user and cluster them, don´t we actually ignore the preferences of the individual user?" What for example happens to a user, who has multiple favorite genres? Imagine a user who listens only to 5 genres in a equally distributed fashion - each of those genres have a score of 0.2 in our previous approach. Now imagine another user who listens to two genres, one 80% and one 20%. At the moment we propose, that the genres with a score of .2 are equally important, eventhrough it is clearly not the case: For user 1, every genre is his favorite, while for user 2 the second genre is not his favorite.

How to bypass this problematic? By scaling our previous results user-individual. That means, we will take a users favorite genre (the max() of his scores), and set it to 1 and scale all other genres accordingly. With this transformation we are able to compare different users.
Going back to the example from above: For user 1, each of his/her 5 genre is now a score of 1 while for user 2 only the first genre is 1 and the second is scaled respectively.


```{r}
profs <- DeezerNew_train_0525 %>% 
  group_by(user_id,alb.genre2_01) %>% #for each user and each genre
  summarise(c = sum(is_listened)) %>% #is_listened is 0/1 coded as.numeric, so sum() works fine
  mutate(p = c/sum(c)) #summarise ungroups once, so at this point we are at groub_by(user_id), therefore sum(c)==sum(c | user(i))

profs <- profs[,-3] %>%
  dcast(user_id ~ alb.genre2_01, value.var="p")
profs[is.na(profs)] <- 0 

scaled <- as.data.frame(t(apply(profs[,-1], 1, function(x)(x-min(x))/(max(x)-min(x))))) #using min(x) instead of 0 for convinience, since for all observations min(x)==0
scaled$user_id <- profs$user_id
scaled[is.na(scaled)] <-0 #for users who have never listened to anything

#creating a lookuptable for final models, better not run it again, it takes ages.

#genre_scaled.molten <- melt(scaled, id.vars="user_id",variable.name="alb.genre2_01",value.name="genre_scaled")
#save(genre_scaled.molten,file="30_05_genre_scaled_lookup.rda")
```

```{r fig.width=8,fig.height=5, message=FALSE}
tmp <- melt(profs)
gg1 <- ggplot(tmp %>% filter(user_id==15980 | user_id==19378 | user_id==10971), aes(x=variable,y=value, fill=user_id))+
  geom_bar(stat="identity", position="dodge")+
  scale_y_continuous(breaks=seq(0,0.65,0.05))+
  scale_fill_brewer(palette="Set1")+
  labs(x="genre",y="relative frequency", title="Relative Genre Frequency",subtitle="for three selected users")+
  theme_light()+
  theme(axis.text.x = element_text(angle = 75, hjust = 1))
ggsave("relative_genre.png",plot=gg1, width = 8, height=5, units="in")

```

```{r fig.width=8,fig.height=5, message=FALSE}
tmp <- melt(scaled)
gg2 <- ggplot(tmp %>% filter(user_id==15980 | user_id==19378| user_id==10971), aes(x=variable,y=value, fill=user_id))+
  geom_bar(stat="identity", position="dodge")+
  scale_y_continuous(breaks=seq(0,1,0.05))+
  
  scale_fill_brewer(palette="Set1")+
  labs(x="genre",y="scaled relative frequency",title="Scaled Relative Genre Frequency",subtitle="for three selected users")+
  theme_light()+
  theme(axis.text.x = element_text(angle = 75, hjust = 1))
ggsave("scaled_relative_genre.png",plot=gg2, width = 8, height=5, units="in")
```


As told, a idea can lead to a cascade of ideas, here is our second thought: "What if the DeezerFlow-Algorithm suggested a genre very often, but it wasn´t listened to often, even through still often enough to let it appear as a favorite genre for this user?" To get a sense for this possibility, we created a new table, with the percentage of how often a genre is listened when played for every user.

```{r}
profs <- DeezerNew_train_0525 %>% 
  group_by(user_id,alb.genre2_01) %>% #for each user and genre
  summarise(c = sum(is_listened),n=n(),p=c/n) #sum(is_listened) again, but this time n() is equal to count() and p as the quotient of c/n

profs <- profs[,-c(3,4)] %>%
  dcast(user_id ~ alb.genre2_01, value.var="p") #long to wide format
profs[is.na(profs)] <- 0 #NAs are equal to 0
listened <- profs[,-1]
listened$user_id <- profs$user_id

#genre_listened.molten <- melt(listened, id.vars="user_id",variable.name="alb.genre2_01",value.name="genre_listened")
#save(genre_listened.molten,file="30_05_genre_listened_lookup.rda")
```

```{r fig.width=8,fig.height=5, message=FALSE}
tmp <- melt(listened)
gg3 <- ggplot(tmp %>% filter(user_id==15980 | user_id==19378 | user_id==10971), aes(x=variable,y=value, fill=user_id))+
  geom_bar(stat="identity", position="dodge")+
  scale_y_continuous(breaks=seq(0,1,0.05),limits = c(0,1))+
  scale_fill_brewer(palette="Set1")+
  labs(x="genre",y="average is_listened",title="Average is_listened per Genre",subtitle="for three selected users")+
  theme_light()+
  theme(axis.text.x = element_text(angle = 75, hjust = 1))
ggsave("average_is_listened.png",plot=gg3, width = 8, height=5, units="in")
```

Combining the scaled genre preferences as well as the listening history is exactly what we had in mind. As one of the value descreases, the overall trustworthyness of the data is going down. As long as both values are high oder extremely low, we can be pretty sure that a user has listened or not listened to the song respectively.

One aspect we havn´t considered yet, is samplesize. Until this point we cannot differentiate between a user who has listened for example to only one song once (scaled value and listenening bahavior is 1) and someone who has heard to a genre hundreds of time.

To compensate this in some way, we extracted this information aswell:

```{r}
profs <- DeezerNew_train_0525 %>% 
  group_by(user_id,alb.genre2_01) %>% #for each user and genre
  summarise(n=n()) %>% #count
  dcast(user_id ~ alb.genre2_01, value.var="n") #long to wide

profs[is.na(profs)] <- 0
counts <- profs[,-1]
counts$user_id <- profs$user_id

#genre_counts.molten <- melt(counts, id.vars="user_id",variable.name="alb.genre2_01",value.name="genre_counts")
#save(genre_counts.molten,file="30_05_genre_counts_lookup.rda")
```


```{r fig.width=8,fig.height=5, message=FALSE}
tmp <- melt(counts)
gg4 <- ggplot(tmp %>% filter(user_id==15980 | user_id==19378| user_id==10971), aes(x=variable,y=value, fill=user_id))+
  geom_bar(stat="identity", position="dodge")+scale_y_continuous(breaks=seq(0,90,5),limits = c(0,90))+
  scale_fill_brewer(palette="Set1")+
  labs(x="genre",y="Count",title="Count per Genre",subtitle="for three selected users")+
  theme_light()+
  theme(axis.text.x = element_text(angle = 75, hjust = 1))
ggsave("count.png",plot=gg4, width = 8, height=5, units="in")
```
test if cols and user_ids are arranged identicalle for each of the three tables.

```{r}
sum(names(listened) != names(scaled)) #should be 0
sum(names(listened) != names(counts))
sum(scaled$user_id!=listened$user_id)
sum(scaled$user_id!=counts$user_id)
```

In our last step we combined the three different values, so show you our thoughtprocess throughout the report, we will show you how the combination changed over some testing and iterative feedback rounds.

To demonstrate what drove the changes, we will create a set of easy example data. Where:

```{r}
g <- c(1,.8,.6,.3) #Genre-Distribution
l <- c(.25,.7,.8,.95) #Listening-History
c <- c(200,68,45,19) #Count
```

Our first approach was g*l*sqrt(c)
using c as a weight to get higher indexvalues when we have more data available. We had never the intention to keep the value inside a 0:1 boundry

```{r}
g*l*sqrt(c)
```

As we can see, there is not enough emphasis on the last case. The last genre has 95% listening rate over 19 ovservations, if this genre would be played again, we could pretty sure predict that the user is going to listen to it.

Additionally case 1 still hase the second highest value, which should indicate a high likelyhood of "is_listened" for a future song of this genre. But the listening value is only 0.25 which indicates, that it is rather unlikely, that the user is going to listen to the genre next time.

In a first step, we exchanged the sqrt() to a log(x+1) function, which gives less weight relatively to the growth in count, the +1 inside the log is needed because log(0) otherwise would lead to -Inf.

```{r}
par(mfrow=c(1,2))
plot(log(1:1000))
plot(sqrt(1:1000))
```

```{r}
g*l*log(c+1)
```

We can see, that the values are closer together now, still, the log is a strong enough penalty for low count cases.
As wanted, case 1 has a lower value than before, since we don´t give him that much weight through his high n.

Since we wanted more emphasis on the listening history, we thought about assigning a weight manualy, something like:
```{r}
weight =5
g*(l*weight)*log(c+1)
```
But tweaking a imaginairy value isn´t what we wanted to do. A more solid way of getting more emphasis on the listening behavior, was to shrink the g by using the sqrt(g). The Squareroot performed well, because g is ranged between 1:0, so sqrt(max(g))==1 and sqrt(min(g))==0 and every value for g which is <1 will get smaller, which in turn is more weight on l. Additionally it has a stronger impact on lower values, which is a benefitial sideeffect.

```{r}
plot(sqrt(seq(1,0,length.out = 100)))
```


```{r}
sqrt(g)*l*log(c+1)
```

In this final version we can see, that case 4 is more important than case 1, but still, due to the small samplesize not too overdone.

Here is the final code, which computes the behavior_index:
```{r}
behavior <- sqrt(scaled[,-47])*listened[,-47]*log(counts[,-47]+1)
behavior$user_id <- scaled$user_id
#behavior_molten <- melt(behavior, id.vars="user_id",variable.name="alb.genre2_01",value.name="behavior")
#save(behavior_molten,file="25_05_new_behavior_lookup.rda")
```

```{r fig.width=8,fig.height=5, message=FALSE}
tmp <- melt(behavior)
gg5 <- ggplot(tmp %>% filter(user_id==15980 | user_id==19378| user_id==10971), aes(x=variable,y=value, fill=user_id))+
  geom_bar(stat="identity", position="dodge")+scale_y_continuous(breaks=seq(0,4,0.25),limits = c(0,4))+
  scale_fill_brewer(palette="Set1")+
  labs(x="genre",y="behavior index",title="Behavior Index",subtitle="for three selected users")+
  theme_light()+
  theme(axis.text.x = element_text(angle = 75, hjust = 1))
ggsave("behavior_index.png",plot=gg5, width = 8, height=5, units="in")
```

```{r fig.width=8,fig.height=5, message=FALSE}
tmp <- cbind.data.frame(sqrt(seq(0,1,length.out = 100)),seq(0,1,length.out = 100))
colnames(tmp) <- c("after","before")
gg6 <- ggplot(tmp,aes(x=before,y=after))+geom_point()+
  scale_y_continuous(breaks=seq(0,1,0.1),limits = c(0,1))+
  scale_x_continuous(breaks=seq(0,1,0.1),limits = c(0,1))+
  scale_fill_brewer(palette="Set1")+
  labs(x="x",y="sqrt(x)",title="Sqrt(x) versus x")+
  theme_light()+
  theme(axis.text.x = element_text(angle = 75, hjust = 1))
ggsave("sqrt.png",plot=gg6, width = 8, height=5, units="in")
```

As you can imagine, the Behavior-Matrix is sparsely filled, since unlistened genres are set to 0, because g will be 0, even if c is not.
Higher values of Behavior_index for a genre is indicating a higher likelyhood of getting heard again by the user, while values close to 0 are indicating a low likelyhood respectively. We used the data for clustering as well as a feature itself. To use it as a feature we melt() the matrix and saved it as a lookuptable, which we will later left join to the data using user_id and genre_id as a key.

```{r}
behavior[1:10,1:10]
```


#### 2(f) User Behavior_index for Clustering users

Finally we can replace the previous clustering by clustering across the users Behavior_index.
If you havn´t read 2(d), you should go back and read the explanations for our clustering approaches and codes.



We want to use this journey back into out beginnings to show how different clustering methods looked like. Finally tho, we used the method "complete" and set the cutoff point to 35 clusters.

```{r}
d <- dist(behavior[,-47]) 
c.tree.comp = hclust(d,method="complete")

w.tree.comp = hclust(d,method="ward.D2")

s.tree.comp = hclust(d,method="single")
```

```{r fig.width=8}
png(filename="clustering_types.png",width=16, height=9, units = "in",res=72)
par(mfrow=c(1,3)) 
plot(c.tree.comp)
plot(w.tree.comp)
plot(s.tree.comp)
dev.off()

png(filename="clustering_complete.png",width=16, height=9, units = "in",res=72)
plot(c.tree.comp)
dev.off()
```

```{r}
tree.comp = hclust(d,method="complete")
labs.comp = cutree(tree.comp,k=35)
cluster_user_lookup <-as.data.frame(behavior$user_id)
cluster_user_lookup$profile_id <- labs.comp
colnames(cluster_user_lookup) <- c("user_id","profile_id")
# save(cluster_user_lookup, file="23_05_SQRT_35_Clusters.rda")
```


```{r}
load("/home/Deezer/30_Wrangled_Data/23_05_SQRT_35_Clusters.rda")
myColors <- c(rep(brewer.pal(7,"Blues")[5:6],17),"#4292C6")

gg13 <- ggplot(cluster_user_lookup, aes(x=profile_id))+stat_bin(binwidth=1,fill=myColors, color="black")+scale_y_log10()+
  scale_x_continuous(breaks=seq(1,35,1))+
labs(y="log10 of observations",
      x="profile cluster",
      title="Logarithmic Distribution of Profile Belonging",
      caption="Source: Feature Engineering Behavior_index Clustering")+
theme_light()+theme(axis.text.x = element_text(angle = 75, hjust = 1))
ggsave("distribution_profiles.png",plot=gg13, width = 8, height=5, units="in")
```

The profiles were saved in a lookuptable and joined onto the data.
Overall, adding the behavior_index as well as behavior_profile increased our accuracy by a significant amount.
We were lucky and could use the data for g, l and c as additional Features.

#### 2(g) Using the Behavior Architecture to Create Further Features

A day before the deadline, after the meeting with Prof. Löcher and to his suggestion we used the existing behavior_index architecture to create new features, not just for genre, but also for artist and album. In this report we set the following code to eval=FALSE, because it is very timeconsuming.
For Example: The lookuptable for artists/user-behavior has roughly 1.3e+09 rows, the lookup for album 2.97e+09 rows.

```{r eval=FALSE}
profs <- DeezerNew_train_0525 %>% 
  group_by(user_id,artist_id) %>% 
  summarise(c = sum(is_listened)) %>% 
  mutate(p = c/sum(c))
profs <- profs[,-3] %>%
  dcast(user_id ~ artist_id, value.var="p")
profs[is.na(profs)] <- 0
scaled <- as.data.frame(t(apply(profs[,-1], 1, function(x)(x-min(x))/(max(x)-min(x)))))
scaled$user_id <- profs$user_id
scaled[is.na(scaled)] <-0 #for users who have never listened to anything

artist_scaled.molten <- melt(scaled, id.vars="user_id",variable.name="artist_id",value.name="artist_scaled")
#save(artist_scaled.molten,file="30_05_artist_scaled_lookup.rda")

profs <- DeezerNew_train_0525 %>% 
  group_by(user_id,artist_id) %>% 
  summarise(c = sum(is_listened),n=n(),p=c/n)
profs <- profs[,-c(3,4)] %>%
  dcast(user_id ~ artist_id, value.var="p")
profs[is.na(profs)] <- 0
listened <- profs[,-1]
listened$user_id <- profs$user_id

artist_listened.molten <- melt(listened, id.vars="user_id",variable.name="artist_id",value.name="artist_listened")
#save(artist_listened.molten,file="30_05_artist_listened_lookup.rda")

profs <- DeezerNew_train_0525 %>% 
  group_by(user_id,artist_id) %>% 
  summarise(n=n()) %>%
  dcast(user_id ~ artist_id, value.var="n")
profs[is.na(profs)] <- 0
counts <- profs[,-1]
counts$user_id <- profs$user_id

artist_counts.molten <- melt(counts, id.vars="user_id",variable.name="artist_id",value.name="artist_counts")
#save(artist_counts.molten,file="30_05_artist_counts_lookup.rda")

behavior_artist <- sqrt(scaled[,-67143])*listened[,-67143]*log(counts[,-67143]+1)
behavior_artist$user_id <- scaled$user_id
behavior_artist_molten <- melt(behavior_artist, id.vars="user_id",variable.name="artist_id",value.name="behavior_artist")
#save(behavior_artist_molten,file="30_05_behavior_artist_lookup.rda")


```

Here is the same code for album_id:

```{r eval=FALSE}
profs <- DeezerNew_train_0525 %>% 
  group_by(user_id,album_id) %>% 
  summarise(c = sum(is_listened)) %>% 
  mutate(p = c/sum(c))
profs <- profs[,-3] %>%
  dcast(user_id ~ album_id, value.var="p")
profs[is.na(profs)] <- 0
scaled <- as.data.frame(t(apply(profs[,-1], 1, function(x)(x-min(x))/(max(x)-min(x)))))
scaled$user_id <- profs$user_id
scaled[is.na(scaled)] <-0 #for users who have never listened to anything

album_scaled.molten <- melt(scaled, id.vars="user_id",variable.name="album_id",value.name="album_scaled")
#save(album_scaled.molten,file="30_05_album_scaled_lookup.rda")

profs <- DeezerNew_train_0525 %>% 
  group_by(user_id,album_id) %>% 
  summarise(c = sum(is_listened),n=n(),p=c/n)
profs <- profs[,-c(3,4)] %>%
  dcast(user_id ~ album_id, value.var="p")
profs[is.na(profs)] <- 0
listened <- profs[,-1]
listened$user_id <- profs$user_id

album_listened.molten <- melt(listened, id.vars="user_id",variable.name="album_id",value.name="album_listened")
#save(album_listened.molten,file="30_05_album_listened_lookup.rda")

profs <- DeezerNew_train_0525 %>% 
  group_by(user_id,album_id) %>% 
  summarise(n=n()) %>%
  dcast(user_id ~ album_id, value.var="n")
profs[is.na(profs)] <- 0
counts <- profs[,-1]
counts$user_id <- profs$user_id

albums_counts.molten <- melt(counts, id.vars="user_id",variable.name="album_id",value.name="album_counts")
#save(album_counts.molten,file="30_05_album_counts_lookup.rda")

behavior_album <- sqrt(scaled[,-ncol(scaled)])*listened[,-ncol(listened)]*log(counts[,-ncol(listened)]+1)
behavior_album$user_id <- scaled$user_id
behavior_album_molten <- melt(behavior_album, id.vars="user_id",variable.name="album_id",value.name="behavior_album")
#save(behavior_album_molten,file="30_05_behavior_album_lookup.rda")

```

Sadly the computation wasn´t fast enough to enable us to get a final prediction with the new features. But we ran some hypertuned models after the competition and will present the results later on.

#### 2(h) Creating the new dataset

*prepare a subset for merging!

```{r Loading new lookup tables}
#Deezer <- read.csv("/home/Deezer/10_Basic_Dataset/train.csv")
#Deezer_test <- read.csv("/home/Deezer/10_Basic_Dataset/test.csv")

#load("/home/Deezer/30_Wrangled_Data/tracks_cleaned.rda") #BPM column
#load("/home/Deezer/30_Wrangled_Data/album_genre_clean.rda") #new_genre
#load("/home/Deezer/30_Wrangled_Data/21_05_new_behavior_lookup.rda")
#load("/home/Deezer/30_Wrangled_Data/23_05_SQRT_35_Clusters.rda")

#load("/home/Deezer/30_05_genre_counts_lookup.rda")
#load("/home/Deezer/30_05_genre_listened_lookup.rda")
#load("/home/Deezer/30_05_genre_scaled_lookup.rda")

#load("/home/Deezer/30_05_artist_listened_lookup.rda")
#load("/home/Deezer/30_05_artist_scaled_lookup.rda")
#load("/home/Deezer/30_05_artist_counts_lookup.rda")

#behavior_molten$user_id <- as.integer(as.character(behavior_molten$user_id))
#cluster_user_lookup$user_id <- as.integer(as.character(cluster_user_lookup$user_id))
```

problems which occured while merging data
```{r}
behavior_molten$user_id <- as.integer(as.character(behavior_molten$user_id))
class(behavior_molten$user_id)

cluster_user_lookup$user_id <- as.integer(as.character(cluster_user_lookup$user_id))

#new genre features
genre_counts.molten$user_id <- as.integer(as.character(genre_counts.molten$user_id))
genre_listened.molten$user_id <- as.integer(as.character(genre_listened.molten$user_id))
genre_scaled.molten$user_id <- as.integer(as.character(genre_scaled.molten$user_id))

#new artist features:
artist_counts.molten$user_id <- as.integer(as.character(artist_counts.molten$user_id))
artist_listened.molten$user_id <- as.integer(as.character(artist_listened.molten$user_id))
artist_scaled.molten$user_id <- as.integer(as.character(artist_scaled.molten$user_id))
artist_counts.molten$artist_id <- as.integer(as.character(artist_counts.molten$artist_id))
artist_listened.molten$artist_id <- as.integer(as.character(artist_listened.molten$artist_id))
artist_scaled.molten$artist_id <- as.integer(as.character(artist_scaled.molten$artist_id))


#Due to the fact that we had problems while removing sample_id, i created a NA column for train data as well. 
Deezer_test$is_listened = 0
#Deezer_test$sample_id = NULL
Deezer$sample_id = NA
```


```{r}
dim(Deezer)
dim(Deezer_test)

#joining data:
get_new_data <- function(x) {
  x <- left_join(x, albums2, by = "album_id", all.x=T, sort=F) #alb.genre_2..
  x <- left_join(x, tracks_clean2, by = "media_id",all.x=T, sort= F) # BPM
  x <- merge(x, behavior_molten, x.by = c("user_id","alb.genre_2_01"),all.x=T,sort=F) #old listening behavior, we 
  x <- left_join(x, cluster_user_lookup, by = "user_id",all.x=T, sort =F) #new clusters
  
  timestamp = as.POSIXct(x$ts_listen, origin="1970-01-01")
  splitdt  <- data.frame(
    hh = as.numeric(format(timestamp, format = "%H")), #24hours format
    wd = as.numeric(format(timestamp, format = "%w"))) #weekday
  x = cbind(x, splitdt) #cbind the extracted time to data
  
  releaseDate = as.Date(as.character(x$release_date), format="%Y%m%d")
  splitrdt  <- data.frame(ryear = as.numeric(format(releaseDate, format = "%Y")))
  x = cbind(x, splitrdt)
  
  #new features (30.05.17)
  x <- merge(x, genre_listened.molten , x.by = c("user_id","alb.genre_2_01"),all.x=T,sort=F)
  x <- merge(x, genre_scaled.molten, x.by = c("user_id","alb.genre_2_01"),all.x=T,sort=F)
  x <- merge(x, genre_counts.molten, x.by = c("user_id","alb.genre_2_01"),all.x=T,sort=F)
  x <- merge(x, artist_counts.molten, x.by = c("user_id","artist_id"),all.x=T,sort=F)
  x <- merge(x, artist_scaled.molten, x.by = c("user_id","artist_id"),all.x=T,sort=F)
  x <- merge(x, artist_listened.molten, x.by = c("user_id","artist_id"),all.x=T,sort=F)
  
  #remove old columns / other columns are deleted below:
  x$genre_id = NULL #delete old genre column
  x$ts_listen = NULL #remove ts_listen
  x$release_date = NULL #remove release_date

  return(x)
}

DeezerNew_train_0621 <- get_new_data(Deezer)
DeezerNew_test_0621 <- get_new_data(Deezer_test)

dim(DeezerNew_train_0621)
dim(DeezerNew_test_0621)

DeezerNew_test_0621 <- DeezerNew_test_0621 %>%
  arrange(sample_id)


#converting specific columns in factors
data_type_function <- function(x){
    cols <- c("media_id", "album_id", "context_type", "platform_name", "platform_family", "listen_type", "user_gender", "user_id", "artist_id", "is_listened", "alb.genre2_01", "wd", "profile_id")
    x[,cols] <- data.frame(apply(x[cols], 2, as.factor))#change data type for these columns into factors
    return(x)
}

DeezerNew_train_0621 <- data_type_function(DeezerNew_train_0621)
DeezerNew_test_0621 <- data_type_function(DeezerNew_test_0621)
#################################
## Unfortunately, we get ~1500 NAs in the DeezerNew_train_0522 dataset and 3 in the test dataset. Therefore, we decided to convert the NAs to 0.
DeezerNew_train_0530$behavior[is.na(DeezerNew_train_0530$behavior)] <- 0
DeezerNew_test_0530$behavior[is.na(DeezerNew_test_0530$behavior)] <- 0

## Add time as circular value
DeezerNew_train_0530$xhh <- sin(2 * pi * DeezerNew_train_0530$hh / 24)
DeezerNew_train_0530$yhh <- cos(2 * pi * DeezerNew_train_0530$hh / 24)
DeezerNew_test_0530$xhh <- sin(2 * pi * DeezerNew_test_0530$hh / 24)
DeezerNew_test_0530$yhh <- cos(2 * pi * DeezerNew_test_0530$hh / 24)

## Delete the hh column
DeezerNew_train_0530 <- subset(DeezerNew_train_0530, select = -hh)
DeezerNew_test_0530 <- subset(DeezerNew_test_0530, select = -hh)

## Combine the files
DeezerNew_traintest_0621 <- rbind(DeezerNew_train_0621,DeezerNew_test_0621)



## Save the new files
save(DeezerNew_train_0621, file = "~/30_Wrangled_Data/Deezer_train_0621.rda")
save(DeezerNew_test_0621, file = "~/30_Wrangled_Data/Deezer_test_0621.rda")
save(DeezerNew_traintest_0621, file = "~/30_Wrangled_Data/Deezer_traintest_0621.rda")  

################
remove_columns <- function(x) {
  x$user_id = NULL 
  x$media_id = NULL 
  x$artist_id = NULL 
  #x$album_id = NULL 
  return(x)
}

```


#Checking Nas and Data order:
```{r}
tail(DeezerNew_traintest_0530)
```

Check NAs for behavior column:
-> The problem is the spelling of ü and ö letters. In the lookup table they are written like: "T<fc>rkischeVolksmusik". So the match is not possible at the moment... 

Hörspiele
DeutscheHörspiele
TürkischeVolksmusik
-> As they are not the most important genres, i think we can fill the NAs with 0. Chris' quick and dirty solution)

```{r}
sum(is.na(DeezerNew_traintest_0530$behavior))

#DeezerNew_test_0523 %>%
  #filter (is.na(DeezerNew_test_0523$behavior))

#Deezer_fail <- DeezerNew_train_0523 %>%
#  filter (is.na(DeezerNew_train_0523$behavior))

#Deezer_fail %>%
 # group_by(alb.genre2_01) %>%
 # summarise (n = n())

#found NAs: ö, ü,... are causing the problems !!!

hist(DeezerNew_traintest_0530$behavior)
```





##3. Modeling

Data scientists spend most of their time on cleaning, labeling and organizing data ([link](https://visit.crowdflower.com/rs/416-ZBE-142/images/CrowdFlower_DataScienceReport.pdf?mkt_tok=eyJpIjoiTTJFek1UaGxNekl5TmpJeCIsInQiOiJPMXpWVnREbHJGR1RHN1BUVk9Zdm5xTEEyNXRpdDRONmZ4XC9oaFwvZGpFNnB0Z0hIWERGS1NqN0huRjB5QUE1UUJVbVZxNFhaeVBSUXJzMTNYdmRJRFhDMU94N). Fortunately, this task has been done. With a solid base of available data, the time has come to build and train our models).

This competition is a classification problem. Therefore, there are several classification algorithms to choose from, e.g. logistic regression, decision trees, boosted trees, k-nearest neighbors, singular value decomposition (SVD) and many more.

As gradient boosted trees are extremely powerful and win many kaggle competitions on a regular basis or as kaggle grandmaster Ben Gorman said, "If linear regression was a Toyota Camry, then gradient boosting would be a UH-60 Blackhawk Helicopter" ([link](https://gormanalysis.com/gradient-boosting-explained/)), we immediately decided to use xgboost as our main library for the gradient boosted tree approach. It has won more than half of all kaggle competitions ([link](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions)).

Furthermore, we also tried a Naive Bayes and Deep Learning approach. Other libraries to build these models include h2o, lightgbm and naivebayes. In the end though, our best model submitted was based on xgboost. Let us take you through the model building process in the following chapter.

#### 3(a) xgboost

Before xgboost can be used, we needed to set up the data one more time. That is because xgboost cannot handle factors. Our dataset included many factors though. There are two ways dealing with this issue. 

As we had many factors in our though, we first needed to one-hot encode the dataset and create a sparse matrix.

## First, we need to load the data and create a sparse matrix to set up XGBoost properly.
```{r setup, include=FALSE}
## 1. Load the data
library(xgboost)
library(Matrix)
library(data.table)

load("~/30_Wrangled_Data/Deezer_train_0530_non_removed.rda")

## Check for NAs because XGBoost cannot deal with them.
summary(DeezerNew_train_0530)  ## No NAs

## 2. Delete artist_id, media_id, album_id, user_id, sample_id and context_type because we created numeric features out of those categorical values. Furthermore, by deleting those factors we decrease the size of our sparse matrix significantly.
DeezerNew_train_0530 <- subset(DeezerNew_train_0530, select = -c(artist_id, media_id, album_id, user_id, sample_id, context_type))

## 3. Create sample (33% of the train data) for CV.
DeezerSampleCV <- DeezerNew_train_0530[sample(nrow(DeezerNew_train_0530), nrow(DeezerNew_train_0530) * .33), ]

## 4. Wrap the sample in a data.table for faster computation.
Deezer_DT <- data.table(DeezerSampleCV, keep.rownames = F)

## 5. Create sparse matrix (leave is_listened out because we want to predict it). -1 deletes the first column because this function genrates a new first column with 1s filled out
sparse_train <- sparse.model.matrix(is_listened ~.-1, data = Deezer_DT)
dim(sparse_train)

## 6. Create an output vector.
output_vector <- DeezerSampleCV[,"is_listened"] == 1
```

## CV on a 1/3 sample of the full dataset. This nested for loop is going to run 1000 different parameter combinations, each combination with 250, 500 and 1000 rounds. The best result will best saved in a CSV file.
```{r}
best_param <- list()
best_seednumber <- 1234
best_logloss <- Inf
best_logloss_index <- 0
best_CVround <- 0

for (iter in 1:1000) {
   param <- list(objective = "binary:logistic",
                 eval_metric = "logloss",
                 max_depth = sample(5:10, 1),
                 eta = runif(1, .01, .3),
                 gamma = runif(1, 0.0, 0.2),
                 subsample = runif(1, .6, .9),
                 colsample_bytree = runif(1, .5, .8),
                 min_child_weight = sample(1:40, 1),
                 max_delta_step = sample(1:10, 1)
                 )
   
   cv_nround <- c(250, 500, 1000)
   cv_nfold <- 5
   seed_number = sample.int(10000, 1)[[1]]
   set.seed(seed_number)
   
   for (validator in cv_nround) {
     mdcv <- xgb.cv(data = sparse_train,
                  label = output_vector,
                  params = param,
                  nfold = cv_nfold,
                  nrounds = validator,
                  nthread = 16,
                  verbose = TRUE,
                  early_stopping_rounds = 15,
                  maximize = FALSE
                  )

   min_logloss <- min(mdcv$evaluation_log$test_logloss_mean)
   min_logloss_index <- which.min(mdcv$evaluation_log$test_logloss_mean)

   if (min_logloss < best_logloss) {
       best_logloss = min_logloss
       best_logloss_index = min_logloss_index
       best_seednumber = seed_number
       best_param = param
       best_CVround = mdcv$niter
       
       }
   }
}

## Save the best parameters
best_parameters <- data.frame(best_param, best_CVround)
write.csv(best_parameters, file = "best_parameters_FinalCV.csv")
```

#### 3(b) ...
#### 3(c) best model

##Reflection

##Team effot



